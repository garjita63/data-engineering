For example, to start a worker and assign only **one CPU core** to it, enter this command:

```
start-worker.sh -c 1 spark://bd-vm:7077
```

![image-20230815111411296](images/image-20230815111411296.png)

Similarly, you can assign a specific amount of memory when starting a worker. The default setting is to use whatever amount of RAM your machine has, minus 1GB.

To start a worker and assign it a specific amount of memory, add the **`-m`** option and a number. For gigabytes, use **`G`** and for megabytes, use **`M`**.

For example, to start a worker with 512MB of memory, enter this command:

```
start-worker.sh -m 512M spark://bd-vm:707
```

![image-20230815111710634](images/image-20230815111710634.png)

## Basic Commands to Start and Stop Master Server and Workers

Below are the basic commands for starting and stopping the Apache Spark master server and workers. Since this setup is only for one machine, the scripts you run default to the localhost.

**To start** **a master** **server** instance on the current machine, run the command we used earlier in the guide:

```
start-master.sh
```

**To stop the master** instance started by executing the script above, run:

```
stop-master.sh
```

**To stop a running worker** process, enter this command:

```
stop-worker.sh
```

You can **start both master and server** instances by using the start-all .spark.sh command:

<u>Note</u>: 

To distinguish between two files with the same name in $HADOOP_HOME/sbin, rename the files name in $SPARK_HOME/sbin :

```
mv $SPARK_HOME/sbin/start-all.sh $SPARK_HOME/sbin/start-all-spark.sh
mv $SPARK_HOME/sbin/stop-all.sh $SPARK_HOME/sbin/stop-all-spark.sh
```

```
start-all-spark.sh
```

![image-20230815113902352](images/image-20230815113902352.png)

Similarly, you **can stop all instances** by using the following command:

```
stop-all-spark.sh
```

### 8.2 Using Spark

#### 8.2.1 Scala (spark-shell)

```
$ cd $SPARK_HOME
$ pwd
/home/bigdata/apache/spark
$ ls
bin   data      jars        LICENSE   NOTICE  R          RELEASE  yarn
conf  examples  kubernetes  licenses  python  README.md  sbin 

Task:
How many lines contain "Spark" in README.md file ?


Solution with spark scala (spark-shell):

$ spark-shell
textFile: org.apache.spark.sql.Dataset[String] = [value: string]

scala> textFile.count() // Number of items in this Dataset
res0: Long = 125                                                                

scala> textFile.first() // First item in this Dataset
res1: String = # Apache Spark

scala> val linesWithSpark = textFile.filter(line => line.contains("Spark"))
linesWithSpark: org.apache.spark.sql.Dataset[String] = [value: string]

scala> textFile.filter(line => line.contains("Spark")).count() // How many lines contain "Spark"?
res2: Long = 20

scala> :q

==> Thera are 20 Spark word in README.md file.
```

#### 8.2.2 Spark (pyspark)

```
>>> textFile = spark.read.text("README.md")
>>> textFile.count()  # Number of rows in this DataFrame
125                                                                             
>>> textFile.first()  # First row in this DataFrame
Row(value='# Apache Spark')
>>> linesWithSpark = textFile.filter(textFile.value.contains("Spark"))
>>> textFile.filter(textFile.value.contains("Spark")).count()  # How many lines contain "Spark"?
20
>>> quit()

==> Thera are 20 Spark word in README.md file.
```
